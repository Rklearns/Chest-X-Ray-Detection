{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ncount = 0\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        count += 1\n        if count >= 10:\n            break\n    if count >= 10:\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:36:09.323994Z","iopub.execute_input":"2025-04-20T18:36:09.324268Z","iopub.status.idle":"2025-04-20T18:36:11.455536Z","shell.execute_reply.started":"2025-04-20T18:36:09.324246Z","shell.execute_reply":"2025-04-20T18:36:11.454806Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/.DS_Store\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/.DS_Store\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1947_bacteria_4876.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1946_bacteria_4875.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1952_bacteria_4883.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1954_bacteria_4886.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1951_bacteria_4882.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1946_bacteria_4874.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/person1949_bacteria_4880.jpeg\n/kaggle/input/chest-xray-pneumonia/chest_xray/chest_xray/val/PNEUMONIA/.DS_Store\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch-geometric\nimport os\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import DataLoader\nfrom torch_geometric.nn import GATConv, global_mean_pool\nfrom torchvision import transforms\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom skimage.segmentation import slic\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:36:14.269665Z","iopub.execute_input":"2025-04-20T18:36:14.270187Z","iopub.status.idle":"2025-04-20T18:36:31.237661Z","shell.execute_reply.started":"2025-04-20T18:36:14.270164Z","shell.execute_reply":"2025-04-20T18:36:31.237028Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.16)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\nlabels = ['PNEUMONIA', 'NORMAL']\n\ndef process_image(args):\n    img, path, label, n_segments = args\n    try:\n        img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n        if img_arr is None:\n            return None\n        resized_arr = cv2.resize(img_arr, (100, 100))  # Downsample\n        \n      \n        img_color = cv2.cvtColor(resized_arr, cv2.COLOR_GRAY2RGB)#this is done for SLIC MAINLT\n        \n        # this generates superpixels using SLIC\n        segments = slic(img_color, n_segments=n_segments, compactness=40, sigma=1)\n\n        #now we will create node features\n        nodes = []\n        valid_indices = []\n        for i in range(np.max(segments) + 1):\n            mask = segments == i\n            if np.sum(mask) > 0:\n                mean_intensity = np.mean(resized_arr[mask])\n                var_intensity = np.var(resized_arr[mask])\n\n                \n                grad_x = cv2.Sobel(resized_arr, cv2.CV_64F, 1, 0, ksize=3)#ye edge intensity nikaalega\n                grad_y = cv2.Sobel(resized_arr, cv2.CV_64F, 0, 1, ksize=3)\n                grad_mag = np.sqrt(grad_x**2 + grad_y**2)\n                edge_intensity = np.mean(grad_mag[mask])\n                if not np.isnan(mean_intensity) and not np.isnan(var_intensity) and not np.isnan(edge_intensity):\n                    nodes.append([mean_intensity / 255.0, var_intensity / 255.0, edge_intensity / 255.0])\n                    valid_indices.append(i)\n        if not nodes:\n            return None\n        nodes = torch.tensor(nodes, dtype=torch.float)\n\n\n        \n        edge_index = []\n        edge_weight = []\n        index_map = {old_idx: new_idx for new_idx, old_idx in enumerate(valid_indices)}\n        unique_segments = np.unique(segments)\n        for seg_id in unique_segments:\n            if seg_id not in valid_indices:\n                continue\n            mask = segments == seg_id\n            dilated = cv2.dilate(mask.astype(np.uint8), np.ones((3, 3), np.uint8), iterations=1)\n            neighbors = np.unique(segments[dilated == 1])\n            for neighbor_id in neighbors:\n                if neighbor_id != seg_id and neighbor_id in valid_indices:\n                    edge_index.append([index_map[seg_id], index_map[neighbor_id]])\n                    edge_index.append([index_map[neighbor_id], index_map[seg_id]])\n                    edge_weight.append(1.0)\n                    edge_weight.append(1.0)\n        if not edge_index:\n            return None\n        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n        edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n        \n      #this is done for data object creation\n        return Data(x=nodes, edge_index=edge_index, edge_attr=edge_weight, y=torch.tensor([labels.index(label)], dtype=torch.long))\n    except:\n        return None\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:36:31.238970Z","iopub.execute_input":"2025-04-20T18:36:31.239492Z","iopub.status.idle":"2025-04-20T18:36:31.249000Z","shell.execute_reply.started":"2025-04-20T18:36:31.239469Z","shell.execute_reply":"2025-04-20T18:36:31.248338Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def get_graph_data(data_dir, n_segments=80, max_images=None):\n    #here we can check out and do tuning that which n_segments give best accuracy \n    \n    data = []#so this will happen to balance the dataset wwe will do this per class\n    for label in labels:\n        path = os.path.join(data_dir, label)\n        images = os.listdir(path)\n        if max_images is not None:\n            images = images[:max_images // len(labels)]  \n        args = [(img, path, label, n_segments) for img in images]\n        with Pool() as pool:\n            results = list(tqdm(pool.imap(process_image, args), total=len(args), desc=f\"Processing {label} in {data_dir}\"))\n        data.extend([r for r in results if r is not None])\n    return data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:36:31.272225Z","iopub.execute_input":"2025-04-20T18:36:31.272424Z","iopub.status.idle":"2025-04-20T18:36:31.285797Z","shell.execute_reply.started":"2025-04-20T18:36:31.272409Z","shell.execute_reply":"2025-04-20T18:36:31.285176Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from multiprocessing import Pool\n\ntrain_data = get_graph_data('/kaggle/input/chest-xray-pneumonia/chest_xray/train')\ntest_data = get_graph_data('/kaggle/input/chest-xray-pneumonia/chest_xray/test')\n\n\n\nprint(f\"Training graphs: {len(train_data)}\")\nprint(f\"Test graphs: {len(test_data)}\")\nprint(f\"Pneumonia train: {sum(g.y.item() for g in train_data)}\")\nprint(f\"Normal train: {len(train_data) - sum(g.y.item() for g in train_data)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:36:31.286394Z","iopub.execute_input":"2025-04-20T18:36:31.286587Z","iopub.status.idle":"2025-04-20T18:38:45.849274Z","shell.execute_reply.started":"2025-04-20T18:36:31.286565Z","shell.execute_reply":"2025-04-20T18:38:45.848330Z"}},"outputs":[{"name":"stderr","text":"Processing PNEUMONIA in /kaggle/input/chest-xray-pneumonia/chest_xray/train: 100%|██████████| 3875/3875 [01:25<00:00, 45.43it/s]\nProcessing NORMAL in /kaggle/input/chest-xray-pneumonia/chest_xray/train: 100%|██████████| 1341/1341 [00:34<00:00, 38.84it/s]\nProcessing PNEUMONIA in /kaggle/input/chest-xray-pneumonia/chest_xray/test: 100%|██████████| 390/390 [00:08<00:00, 46.84it/s]\nProcessing NORMAL in /kaggle/input/chest-xray-pneumonia/chest_xray/test: 100%|██████████| 234/234 [00:05<00:00, 42.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Training graphs: 5216\nTest graphs: 624\nPneumonia train: 1341\nNormal train: 3875\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class ChestXrayGraphDataset(Dataset):\n    def __init__(self, graph_list, augment=False):\n        self.graph_list = graph_list\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.graph_list)\n\n    def __getitem__(self, idx):\n        graph = self.graph_list[idx]\n        if torch.isnan(graph.x).any():\n            print(f\"Invalid graph at index {idx}\")\n            return self.__getitem__((idx + 1) % len(self.graph_list))\n        \n        if self.augment:\n            # Applyint graph augmentation\n            if np.random.random() > 0.5:\n               \n                noise = torch.randn_like(graph.x) * 0.05\n                graph.x = graph.x + noise\n                graph.x = torch.clamp(graph.x, 0.0, 1.0) \n        return graph\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:39:28.353176Z","iopub.execute_input":"2025-04-20T18:39:28.353481Z","iopub.status.idle":"2025-04-20T18:39:28.359301Z","shell.execute_reply.started":"2025-04-20T18:39:28.353458Z","shell.execute_reply":"2025-04-20T18:39:28.358434Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class GNNModel(nn.Module):\n    def __init__(self, input_dim=3, hidden_dim=64, dropout_rate=0.3):\n        super(GNNModel, self).__init__()\n        \n        self.conv1 = GATConv(input_dim, hidden_dim, heads=4)\n        self.bn1 = nn.BatchNorm1d(hidden_dim * 4)\n        \n        \n        self.conv2 = GATConv(hidden_dim * 4, hidden_dim * 2, heads=2)\n        self.bn2 = nn.BatchNorm1d(hidden_dim * 2 * 2)\n        \n        \n        self.conv3 = GATConv(hidden_dim * 2 * 2, hidden_dim, heads=1)\n        self.bn3 = nn.BatchNorm1d(hidden_dim)\n    \n        self.fc1 = nn.Linear(hidden_dim, 64)\n        self.fc2 = nn.Linear(64, 1)\n        self.dropout = nn.Dropout(dropout_rate)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n       \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n    \n    \n        x = self.conv1(x, edge_index)\n        x = self.bn1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.dropout(x)\n     \n        x = self.conv2(x, edge_index)\n        x = self.bn2(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.dropout(x)\n        \n      \n        x = self.conv3(x, edge_index)\n        x = self.bn3(x)\n        x = F.leaky_relu(x, 0.2)\n        \n      \n        x = global_mean_pool(x, batch)\n    \n     \n        x = self.fc1(x)\n        x = F.leaky_relu(x, 0.2)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:45:12.276043Z","iopub.execute_input":"2025-04-20T18:45:12.276318Z","iopub.status.idle":"2025-04-20T18:45:12.284436Z","shell.execute_reply.started":"2025-04-20T18:45:12.276298Z","shell.execute_reply":"2025-04-20T18:45:12.283658Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def evaluate(model, loader, criterion):\n    model.eval()\n    correct = 0\n    total = 0\n    total_loss = 0\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for data in loader:\n            data = data.to(device)\n            out = model(data).squeeze()\n            loss = criterion(out, data.y.float())\n            total_loss += loss.item()\n            \n            pred = (out > 0).float()\n            correct += pred.eq(data.y.float()).sum().item()\n            total += data.y.size(0)\n            \n            predictions.extend(pred.cpu().numpy())\n            true_labels.extend(data.y.cpu().numpy())\n    \n    return total_loss / len(loader), correct / total\n    \n ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:59:26.265383Z","iopub.execute_input":"2025-04-20T18:59:26.266001Z","iopub.status.idle":"2025-04-20T18:59:26.271492Z","shell.execute_reply.started":"2025-04-20T18:59:26.265976Z","shell.execute_reply":"2025-04-20T18:59:26.270801Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def train_with_early_stopping():\n   \n    train_dataset = ChestXrayGraphDataset(train_data, augment=True)\n    test_dataset = ChestXrayGraphDataset(test_data, augment=False)\n    \n   \n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n    model = GNNModel().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n    \n    #class wts are calci for class imbalance\n    num_pneumonia = sum(g.y.item() == 0 for g in train_data)\n    num_normal = sum(g.y.item() == 1 for g in train_data)\n    weight = torch.tensor([num_normal / len(train_data), num_pneumonia / len(train_data)]).to(device)\n    \n    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([num_normal / num_pneumonia]).to(device))\n    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-6, verbose=True)\n    \n    \n    epochs = 20\n    best_val_acc = 0\n    patience = 4  # Early stopping patience\n    patience_counter = 0\n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n    \n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        correct = 0\n        total = 0\n        \n        for data in train_loader:\n            data = data.to(device)\n            optimizer.zero_grad()\n            out = model(data).squeeze()\n            loss = criterion(out, data.y.float())\n            loss.backward()\n            \n            # Gradient clipping to prevent exploding gradients(this is one step that i found very useful)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            total_loss += loss.item()\n            \n    \n            pred = (out > 0).float()\n            correct += pred.eq(data.y.float()).sum().item()\n            total += data.y.size(0)\n        \n        train_loss = total_loss / len(train_loader)\n        train_acc = correct / total\n        \n        val_loss, val_acc = evaluate(model, test_loader, criterion)#we will\n        \n        \n        scheduler.step(val_acc)#this is done to update learning scheduler\n        \n       \n        history['train_loss'].append(train_loss)#ye history ke liye models ka\n        history['train_acc'].append(train_acc)\n        history['val_loss'].append(val_loss)\n        history['val_acc'].append(val_acc)\n        \n        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n        \n        # Early stopping check\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            # we wil save the best model\n            torch.save(model.state_dict(), 'best_gnn_model.pth')\n            print(f\"Saved new best model with validation accuracy: {val_acc:.4f}\")\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n\n    \n    # Load the best model for evaluation\n    model.load_state_dict(torch.load('best_gnn_model.pth'))\n    \n\n    print(\"\\nFinal evaluation with the best model:\")\n    test_loss, test_acc = evaluate(model, test_loader, criterion)\n    print(f\"Final Test Accuracy: {test_acc * 100:.2f}%\")\n    \n    return model, history, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:59:28.370133Z","iopub.execute_input":"2025-04-20T18:59:28.370728Z","iopub.status.idle":"2025-04-20T18:59:28.381028Z","shell.execute_reply.started":"2025-04-20T18:59:28.370707Z","shell.execute_reply":"2025-04-20T18:59:28.380286Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def get_final_classification_report(model, test_loader):\n    model.eval()\n    predictions = []\n    true_labels = []\n    \n    with torch.no_grad():\n        for data in test_loader:\n            data = data.to(device)\n            out = model(data).squeeze()\n            pred = (out > 0).float()\n            predictions.extend(pred.cpu().numpy())\n            true_labels.extend(data.y.cpu().numpy())\n    \n    accuracy = np.mean(np.array(predictions) == np.array(true_labels))\n    print(f\"Final Test Accuracy: {accuracy * 100:.2f}%\")\n    \n    print(\"\\nTHE CLASSIFICATION REPORT OF GNN IS AS FOLLOWS:\")\n    print(classification_report(true_labels, predictions, target_names=['Pneumonia', 'Normal']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T18:59:31.257209Z","iopub.execute_input":"2025-04-20T18:59:31.257484Z","iopub.status.idle":"2025-04-20T18:59:31.263032Z","shell.execute_reply.started":"2025-04-20T18:59:31.257466Z","shell.execute_reply":"2025-04-20T18:59:31.262304Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\nmodel,history, test_loader = train_with_early_stopping()\nget_final_classification_report(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T19:04:00.886637Z","iopub.execute_input":"2025-04-20T19:04:00.886937Z","iopub.status.idle":"2025-04-20T19:04:22.564324Z","shell.execute_reply.started":"2025-04-20T19:04:00.886917Z","shell.execute_reply":"2025-04-20T19:04:22.563641Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Train Loss: 0.1827, Train Acc: 0.8052, Val Loss: 0.7105, Val Acc: 0.6987\nSaved new best model with validation accuracy: 0.6987\nEpoch 2/20, Train Loss: 0.1668, Train Acc: 0.8150, Val Loss: 0.2315, Val Acc: 0.8269\nSaved new best model with validation accuracy: 0.8269\nEpoch 3/20, Train Loss: 0.1673, Train Acc: 0.8190, Val Loss: 0.2610, Val Acc: 0.8205\nEpoch 4/20, Train Loss: 0.1664, Train Acc: 0.8192, Val Loss: 0.4555, Val Acc: 0.7772\nEpoch 5/20, Train Loss: 0.1644, Train Acc: 0.8152, Val Loss: 0.3871, Val Acc: 0.7821\nEpoch 6/20, Train Loss: 0.1620, Train Acc: 0.8294, Val Loss: 0.3572, Val Acc: 0.8077\nEarly stopping at epoch 6\n\nFinal evaluation with the best model:\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/4189575232.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_gnn_model.pth'))\n","output_type":"stream"},{"name":"stdout","text":"Final Test Accuracy: 82.69%\nFinal Test Accuracy: 82.69%\n\nTHE CLASSIFICATION REPORT OF GNN IS AS FOLLOWS:\n              precision    recall  f1-score   support\n\n   Pneumonia       0.83      0.91      0.87       390\n      Normal       0.82      0.69      0.75       234\n\n    accuracy                           0.83       624\n   macro avg       0.83      0.80      0.81       624\nweighted avg       0.83      0.83      0.82       624\n\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"# WE HAVE FINALLY DONE IT!!! \n# GOT TEST ACCURACY OF 82.69 Through this GNN Model","metadata":{}},{"cell_type":"markdown","source":"# LEARNINGS\nI explored a bunch of stuff online how I can get better accuracy through different models\nWhat I did???\nGoogle,Perplexity,stack overflow, documentation-> explored how can i get better accuracy through GNN and not CNN, resnet although I think CNN gives WAYYYYYY BETERRR RESULTS!!\n  # here is what I learned through GNN\n  1. GTA Over Gnc-> believe me it turned out to be game changer\n  2. Tuning of n_Segments\n  3. WHAT MISTAKE I WAS MAKING???-> not taking the whole dataset and setting max_images\n  4. Weighting the loss function to control the imbalance of the dataset\n  5. Gradient clipping was essential to stabilize training and avoid exploding gradients.\n  6. Did some hyperparameter tuning with GAT heads\n  7. Early stopping and learning rate scheduling ensured optimal convergence without overfitting.\n  8. I was stuck at 65-70 with this dataset with GNN Model but after these changes it gives good accuracy","metadata":{}}]}